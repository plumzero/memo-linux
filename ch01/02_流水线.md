
### 工作原理

一条指令的执行一般要经过取指令、翻译指令、执行指令 3 个基本流程。CPU 内部的电路分为不同的单元: 取指单元、译码单元、执行单元等，指令的执行也是按照流水线工序一步一步执行的。假设每一个步骤的执行时间都是一个时钟周期，那么一条指令执行完需要 3 个时钟周期。

CPU 执行指令的 3 个时钟周期里，取指单元只在第一个时钟周期里工作，其余两个时钟周期都处理空闲状态，其他两个执行单元也是如此。引入流水线后，可以让三个单元都动作起来，虽然每一条指令的执行流程和时间不变，但是从整条流水线的输出来看，差不多平均每个时钟周期就能执行一条指令。

流水线的本质其实就是拿空间换时间。将每条指令分解为多步执行，指令的每一小步都有独立的电路单元来执行，并让不同指令的各小步操作重叠，通过多条指令的并行执行，加快程序的整体运行效率。

### 超流水线技术

流水线中耗时最长的那道工序单元的执行时间(即时间延迟)决定了 CPU 流水线的性能，对其再进行细分，拆解为更多工序，这样流水线越深，每一道工序的执行时间就会变得越小，处理器的时钟周期就可以更短，CPU 的工作频率就可以更高，进而可以提升 CPU 的性能。

CPU 内部的数字电路是靠时钟驱动来工作的，一个时钟周期的时间变短，CPU 主频也就相应提升，影响时钟周期时间长短的一个关键的制约因素就是 CPU 内部每一个工序执行单元的耗费时间。所以提升 CPU 主频，本质在于减少流水线中每一级流水的执行时间。解决办法有三个:
- 优化流水线中各级流水线的性能，受限于当前集成电路的设计水平，这步最难。
- 依靠半导体制造工艺，制程越先进，芯片面积就越小，发热也就越小，就更容易提升主频。
- 不断地增加流水线深度，流水线越深，流水线中的各级时间延迟就可以做得越小，就更容易提高主频。

流水线不一定越深越好。一方面，流水线越深，电路会越复杂，就需要更多的组合逻辑电路和寄存器，芯片面积也就越大，功耗也就随之上升了。另一方面，流水线越深也未必越能够提升性能。流水线是靠指令的并行来提升性能的，执行的程序指令如果是顺序结构的，没有中断或跳转，流水线确定可以提高执行效率。但是当程序指令中存在跳转、分支结构时，下面预取的指令可能就要全部丢掉了，需要到跳转的地方重新取指令执行。

流水线越深，一旦预取指令失败，浪费和损失就会越严重，因为流水线中预取的几十指令可能都要丢弃掉，此时流水线就发生了停顿，无法按照预期继续执行，这种情况一般称为流水线冒险(hazard)。

### 流水线冒险

引起流水线冒险的原因有很多种，根据类型不同，一般分为 3 种。
- 结构冒险: 所需的硬件(如内存单元、寄存器等)正在为前面的指令工作。
- 数据冒险: 当前指令需要前面指令的运算数据才能执行。
- 控制冒险: 需根据之前指令的执行结果决定下一步的行为。

### 分支预测

现在的 CPU 流水线在取指和译码时，都要对跳转指令进行分析，预测可能执行的分支和路径，防止预取错误的分支路径指令给流水线带来停顿。

根据工作方式的不同，分支预测可分为静态预测和动态预测。静态预测在程序编译时通过编译器进行分支预测，这种预测方式对于循环程序最有效，它可以根据你的循环边界反复取指令。而对于跳转分支，静态预测一般都是默认不跳转，按照顺序执行。所以在编写有跳转分支的程序时，把大概率执行的代码分支放在前面，可以明显提高代码的执行效率。

动态预测则指在程序运行时进行预测。不同的软件、不同的程序分支行为，可以采取不同的算法去提高预测的准确率。如可以根据程序的历史执行路径信息来预测本次跳转的行为。

分支预测技术是提高 CPU 性能的一项关键技术，其本质就是去除指令之间的相关性，让程序更高效运行。一个分支预测器好不好，可以从两个方面来衡量: 分支判断速度和预测准确率。目前分支预测技术可以达到 95% 的预测准确率，然而技术进化之路永未停止，分支预测技术一直在随着计算机的发展不断更新迭代。

### 乱序执行

编写后的代码指令序列按照顺序依次存储在 RAM 中。当程序执行时，PC 指令会自动到 RAM 中去取，然后 CPU 按照顺序一条一条地依次执行，这种执行方式称为顺序执行(in order)，当这些指令前后有数据依赖关系时，就会产生数据冒险，可以通过在指令序列之间添加空指令，让流水线暂时停顿来避免流水线中预期的指令被冲刷掉。除此之外，还可以通过乱序执行(out of order)来避免流水线冲突。

造成流水线冲突的根源在于指令之间存在相关性: 前后指令之间要么产生数据冒险，要么产生结构冒险。通过重排指令的执行顺序，在一定程度上可以去掉这种依赖。

支持乱序执行的 CPU 处理器，其内部一般都会有专门的乱序执行逻辑电路，该控制电路会对当前指令的执行序列进行分析，看能否提前执行。如整型计算、浮点型计算会使用不同的计算单元，同时执行这些指令并不会发生冲突。CPU 分析这些不相关的指令，并结合各电路单元的空闲状态综合判断，将能提前执行的指令进行重排，发送到相应的电路单元执行。

### SIMD

一条指令一般由操作码和操作数构成，不同类型的指令，其操作数的数量可能不一样。当译码电路成功并开始操作指令时， CPU 控制单元会首先从内存中取数据，将操作数送到算术逻辑单元中，取数据的方法有两种: 第一种是先取第一个操作数，然后访问内存读取第二个操作数，最后才进行计算。这种数据操作类型一般称为单指令单数据(Single Instruction Single Data,SISD)；第二种方法是几个执行部件同时访问内存，一次性读取所有的操作数，这种数据操作类型称为单指令多数据(Single Instruction Multiple Data,SIMD)。SIMD 通过单指令多数据运算，帮助 CPU 实现了数据并行访问，SIMD 型的 CPU 执行效率更高。

随着多媒体技术的发展，计算机对图像、视频、音频等数据的处理需求大增，SIMD 特别适合这种数据密集型计算: 一条指令可以同时处理多个数据(音频或一帧图像数据)。多媒体扩展(MultiMedia eXtensions,MMX)指令集是 X86 处理器为音视频、图像处理专门设计的 57 条 SIMD 多媒体指令集。SSE(Internet Streaming SIMD Extensions)指令集是 Intel 在奔腾三处理器中对 MMX 进行扩展的指令集。ARM 架构的处理器也开始慢慢支持和扩展 SIMD 指令集。NEON 是适用于 Cortex-A 和 Cortex-R52 系列处理器的一种 128 位的 SIMD 扩展指令集。

### 单发射和多发射

每个时钟周期只能从存储器取一条指令，每个时钟周期也只能执行一条指令，这种处理器一般叫做单发射处理器。

多发射处理器在一个时钟周期内可以执行多条指令。处理器内部一般有多个执行单元，如算术逻辑单元(ALU)、乘法器、浮点计算单元(FPU)等，多发射处理器可以在一个时钟周期内同时分发(dispatch)多条指令到不同的执行单元运行，让 CPU 同时执行不同的计算(加法、乘法、浮点运算等)，充分利用执行单元，从而达到指令级的并行。一个双发射处理器每个时钟周期理论上最多可执行 2 条指令，一个四发射处理器每个时钟周期理论上最多可以执行 4 条指令。

根据实现方式的不同，多发射处理器又可分为静态发射和动态发射。静态发射指在编译阶段将可以并行的指令打包，合并到一个 64 位的长指令中。在打包过程中，若找不到可以并行的指令配对，则用空指令 NOP 补充。这种实现方式称为超长指令集架构(Very Long Instruction Word,VLIW)。

VLIW 实现简单，不需要额外的硬件，通过编译器在编译阶段就可以完成指令的并行。随着处理器不断地迭代更新，为了保证指令集的兼容性，现在的处理器，如 X86、ARM 等都采用 SuperScalar 结构。采用 SuperScalar 结构的处理器又叫超标量处理器。这种处理器在多发射的实现过程中会增加额外的取指单元、译码单元、逻辑控制单元等硬件电路。在指令运行时，将串行的指令序列转换为并行的指令序列，分发到不同的执行单元去执行，通过指令的动态并行化来提供 CPU 的性能。

VLIW 和 SuperScalar 分别从编译器和硬件上实现了指令的并行化，各有各的优势和局限性。VLIW 虽然实现简单，但由于兼容性问题，不支持目前主流的 X86、ARM 处理器；而采用 SuperScalar 结构的处理器，完全依赖流水线硬件去动态识别可并行执行的指令，并分发到对应的执行单元执行，不仅大大增加了硬件电路的复杂性，而且也存在极限。

现在新架构的处理器一般会采用显式并行指令计算(Explicitly Parallel Instruction Computing,EPIC)的指令集结构，它结合了 VLIW 和 SuperScalar 的优点，允许处理器根据编译器的调度并行执行指令而不增加硬件的复杂性。
